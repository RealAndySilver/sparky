{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(date_text):\n",
    "    \"\"\"Get timestamp from date text\"\"\"\n",
    "    try:\n",
    "        date_text = datetime.strptime(date_text, '%Y/%m/%d %H:%M:%S+00')\n",
    "    except ValueError:\n",
    "        date_text = datetime.strptime(\n",
    "            \"1900/01/01 01:01:01+00\", '%Y/%m/%d %H:%M:%S+00')\n",
    "\n",
    "    return date_text.timestamp()\n",
    "\n",
    "def snap_time_to_resolution(timestamp, resolution=1):\n",
    "    \"\"\"Snap time to resolution\"\"\"\n",
    "    if resolution <= 0:\n",
    "        resolution = 1\n",
    "    resolution_ms = resolution * 60\n",
    "    snapped_time = datetime.fromtimestamp(\n",
    "        math.floor(timestamp / resolution_ms) * resolution_ms)\n",
    "\n",
    "    return snapped_time\n",
    "\n",
    "def snap_row(date, resolution=5):\n",
    "    \"\"\"Snap row to resolution\"\"\"\n",
    "    timestamp = get_timestamp(date)\n",
    "    snapped_time = snap_time_to_resolution(timestamp, resolution)\n",
    "    return snapped_time\n",
    "\n",
    "snap_udf = udf(snap_row, 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OBJECTID: integer (nullable = true)\n",
      " |-- DCH_FEA_KEY: integer (nullable = true)\n",
      " |-- DCH_GRPH_KEY: integer (nullable = true)\n",
      " |-- DCH_FEATYPE_TEXT: string (nullable = true)\n",
      " |-- DCH_OWNER_NAME: string (nullable = true)\n",
      " |-- DCH_PRBL_FLOW_TYPE: string (nullable = true)\n",
      " |-- DCH_MATERIAL_TYPE: string (nullable = true)\n",
      " |-- DCH_PIPE_SHP_TEXT: string (nullable = true)\n",
      " |-- DCH_LIFECYCLE_STAT: string (nullable = true)\n",
      " |-- DCH_DSTNTN_TYPE: string (nullable = true)\n",
      " |-- DCH_INSPECT_FLAG: string (nullable = true)\n",
      " |-- DCH_CASING_FLAG: string (nullable = true)\n",
      " |-- DCH_PERF_PIPE_FLAG: string (nullable = true)\n",
      " |-- DCH_LENGTH_FT_NBR: double (nullable = true)\n",
      " |-- DCH_WIDTH_IN_NBR: integer (nullable = true)\n",
      " |-- DCH_HEIGHT_IN_NBR: integer (nullable = true)\n",
      " |-- DCH_UPS_ELEV_FT_NBR: double (nullable = true)\n",
      " |-- DCH_DNS_ELEV_FT_NBR: double (nullable = true)\n",
      " |-- DCH_INSTALL_DATE: string (nullable = true)\n",
      " |-- DCH_LST_UPDT_DATE: string (nullable = true)\n",
      " |-- DCH_STREAM_NAME: string (nullable = true)\n",
      " |-- DCH_GSIP_NAME: string (nullable = true)\n",
      " |-- SHAPE_Length: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_df = spark.read.csv('data/underdrain.csv',\n",
    "                         header=True,\n",
    "                         inferSchema=True)  # type: ignore\n",
    "\n",
    "# data_df.na.drop(subset=['DCH_INSTALL_DATE']).show(truncate=False)\n",
    "data_df.printSchema()\n",
    "filtered_df = data_df.filter(data_df.DCH_INSTALL_DATE.isNotNull())\n",
    "\n",
    "# filtered_df.foreach(lambda row: snap_row(row[18], 1))  # type: ignore\n",
    "\n",
    "# filtered_df.withColumn('SNAPPED_TIME', snap_row(filtered_df.DCH_INSTALL_DATE, 1)).show(5)    # type: ignore\n",
    "filtered_df.withColumn('SNAPPED_TIME', snap_udf(filtered_df.DCH_INSTALL_DATE)).write.csv(\"data/csvs\", header=True)\n",
    "# filtered_df.show(10)\n",
    "# filtered_df.write.csv(\"data/csvs\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
